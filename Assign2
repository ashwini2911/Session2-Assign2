1. HDFS-
HDFS is Hadoop Distributed File System which is a file system to manage the BigData processes by hadoop. Hadoop has two basic parts which are the challenges for big data HDFS and Map Reduce. HDFS takes care of storage part for Hadoop.HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode. HDFS stores large files in blocks in the cluster.It stores each file as a block of similar size. The default size for a block is 128 MB. All the blocks are of same size except the last block.HDFS is responsible to take care of how the data is being stored.

2. Hadoop Cluster-
Hadoop cluster is special type of computational uint which is designed to store and process large amount of data which may be structured or unstructured. Clusters run on low cost commodity machines. e machine in the cluster is designated as the NameNode and another machine the as JobTracker; these are the masters. The rest of the machines in the cluster act as both DataNode and TaskTracker; these are the slaves. Hadoop clusters are often referred to as "shared nothing" systems because the only thing that is shared between nodes is the network that connects them.A machine in the cluster is designated as the NameNode and another machine the as JobTracker; these are the masters. The rest of the machines in the cluster act as both DataNode and TaskTracker; these are the slaves. Hadoop clusters are often referred to as "shared nothing" systems because the only thing that is shared between nodes is the network that connects them.
A Hadoop cluster is useless untill it has data so we begin by loading large files on Hadoop clustrers. This large files are then broken into a fixed size blocks and separated onto multiple machines in the cluster. Then these blocks are processed separately on multiple machines in a cluster which is called data nodes and instructions in the clusters are managed by master node of the cluster.
There are following advanages pof clustered architecture.
a. Clustering is a very popular technic among Sys-Engineers that they can cluster servers as a failover system, a load balance system or a parallel processing unit.
b. Clustering servers is completely a scalable solution. You can add resources to the cluster afterwards.
c. If a server in the cluster needs any maintenance, you can do it by stopping it while handing the load over to other servers.
d. Among high availability options, clustering takes a special place since it is reliable and easy to configure. In case of a server is having a problem providing the services furthermore, other servers in the cluster can take the load.

3. HDFS Block-
HDFS blocks are different from normal file system block. File system blocks are smallest unit of data that can be stored or retrieved from the disk. Filesystem blocks are normally in few kilobytes of size. Blocks are transparent to the user who is performing filesystem operations like read and write.
Whereas HDFS blocks are very large in size although the concept of blocks is same as file system of traditional systems. Default size of HDFS block is 64 MB. The files are split into 64 MB blocks and then stored into the hadoop filesystem. The hadoop application is responsible for distributing the data blocks across multiple nodes. he main reason for having the HDFS blocks in large size is to reduce the cost of seek time. 
Advantages of having blocked structure
a. The blocks are of fixed size, so it is very easy to calculate the number of blocks that can be stored on a disk.
b. HDFS block concept simplifies the storage of the datanodes. The datanodes doesn’t need to concern about the blocks metadata data like file permissions etc. The namenode maintains the metadata of all the blocks.
c. If the size of the file is less than the HDFS block size, then the file does not occupy the complete block storage.
d. As the file is chunked into blocks, it is easy to store a file that is larger than the disk size as the data blocks are distributed and stored on multiple nodes in a hadoop cluster.
e. Blocks are easy to replicate between the datanodes and thus provide fault tolerance and high availability. Hadoop framework replicates each block across multiple nodes (default replication factor is 3). In case of any node failure or block corruption, the same block can be read from another node.
